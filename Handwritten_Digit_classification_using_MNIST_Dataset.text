# Handwritten-Digit-classification-using-MNIST-Datas-
This will recognise the digits in your file
import tensorflow as tf
    import numpy as np
    from tensorflow import keras
    print(tf.__version__)
 o/p:2.3.0
   
   class mycallbacks(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc') > 0.99):
      print("\nStops the Training.")
      self.model.stop_training = True
      
   data = tf.keras.datasets.mnist
   
   data
 o/p: <module 'tensorflow.keras.datasets.mnist' from '/usr/local/lib/python3.6/dist-packages/tensorflow/keras/datasets/mnist/__init__.py'>
 
   (train_imgs, train_labels), (test_imgs, test_labels) = data.load_data()
 
   print(len(train_imgs))
   print(len(test_imgs))
o/p:60000
    10000
   
   import matplotlib.pyplot as plt
plt.imshow(train_imgs[600])
print(np.array(train_labels[600]))
print(np.array(train_imgs[600]))

o/p:9
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0  16 170 255 254 219  12
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  29 238 254 252 245 253 142
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  25 238 254 230  75   0 162 195
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 173 254 215  13   0  20 211 218
   10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 196 240  30   0   0  64 254 249
   23   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0  19 240 157   0   5 111 245 254 159
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0  26 254 124  97 191 254 254 254  48
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   6 208 254 254 254 254 254 247  25
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  85 246 248 147 100 254 199   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  36  39   0  46 254 118   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  62 254  33   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 173 244  27   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   9 221 186   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0  42 254 102   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 118 254  37   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 147 254  37   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 209 211   3   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 209 155   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 209 125   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 181  70   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]]

callbacks = mycallbacks()

train_imgs = train_imgs.reshape(60000, 28, 28, 1)
train_imgs = train_imgs / 255

test_imgs = test_imgs.reshape(10000, 28, 28, 1)
test_imgs = test_imgs / 255

print(train_imgs.shape)
o/p:(60000, 28, 28, 1)

print(train_imgs)
o/p:[[[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]
   .
   .
   .
   [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]]]
   
   model = tf.keras.models.Sequential([
                                    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu', input_shape = (28, 28, 1)),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation = 'relu'),
                                    tf.keras.layers.Dense(10, activation = 'softmax')
                                   
   model.compile(optimizer = 'Adam',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
              
   model.summary()
   o/p:Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 128)       1280      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 128)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        73792     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               204928    
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 281,290
Trainable params: 281,290
Non-trainable params: 0

model.fit(train_imgs, train_labels, epochs = 20)
o/p:Epoch 1/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0462 - accuracy: 0.9855
Epoch 2/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0341 - accuracy: 0.9894
Epoch 3/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0248 - accuracy: 0.9921
Epoch 4/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0198 - accuracy: 0.9937
Epoch 5/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0164 - accuracy: 0.9943
Epoch 6/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0138 - accuracy: 0.9951
Epoch 7/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0126 - accuracy: 0.9958
Epoch 8/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0120 - accuracy: 0.9961
Epoch 9/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0087 - accuracy: 0.9971
Epoch 10/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0101 - accuracy: 0.9966
Epoch 11/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0095 - accuracy: 0.9969
Epoch 12/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0078 - accuracy: 0.9974
Epoch 13/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0074 - accuracy: 0.9977
Epoch 14/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0066 - accuracy: 0.9977
Epoch 15/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0072 - accuracy: 0.9978
Epoch 16/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0070 - accuracy: 0.9976
Epoch 17/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0059 - accuracy: 0.9980
Epoch 18/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0052 - accuracy: 0.9983
Epoch 19/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0057 - accuracy: 0.9985
Epoch 20/20
1875/1875 [==============================] - 17s 9ms/step - loss: 0.0058 - accuracy: 0.9982
<tensorflow.python.keras.callbacks.History at 0x7fb31ab88ba8>

model.evaluate(test_imgs, test_labels)
o/p:313/313 [==============================] - 1s 4ms/step - loss: 0.0431 - accuracy: 0.9907
[0.04310772567987442, 0.9907000064849854]

prediction = model.predict([test_imgs], batch_size = 128)

output_label = np.argmax(prediction[100])

print(output_label)
o/p:6
 
 
 
 
      
      
